<!doctype html>
<meta charset="utf-8">
<script src="https://distill.pub/template.v1.js"></script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {inlineMath: [["$","$"],["\\(","\\)"]]}
  });
</script>
<style>
video {
  width: 100%    !important;
  height: auto   !important;
} 
</style>
<script type="text/javascript" src="https://www.tuhh.de/MathJax/MathJax.js?config=TeX-AMS_HTML-full"></script>

<script type="text/front-matter">
  title: "Human Behavior Imitation Learning"
  description: "Description of the post"
  authors:
  - Shoubhik Debnath: http://colah.github.io
  - Arnout Devos: http://shancarter.com
  - Eric Heiden: http://shancarter.com
  - Ryan Julian: http://shancarter.com
  - Fiona Khatana: http://shancarter.com
  affiliations:
  - USC: http://usc.edu
  - USC: http://usc.edu
  - USC: http://usc.edu
  - USC: http://usc.edu
  - USC: http://usc.edu
</script>

<dt-article class="centered" style="text-align:justify">
  <h1>Humanoid Imitation Learning from Video</h1>
    <h2 style="text-align:justify">This article describes a system which demonstrates imitation learning for useful skills on humanoid figures. The system employs a Generative Adversarial Imitation Learning model and is able to imitate directly from videos of human skill, without explicit reward.</h2>
  <dt-byline></dt-byline>
  <p>
    Intro + GAIL <font color="red">(Needs work)</font>
  </p>
  <p>
    $$\min_{\theta} \mathrm{E}_{s,a \sim p(s,a)}[L(D_{\theta}(s,a), 1)] + \mathrm{E}_{z \sim q(z)}[L(D_{\theta}(G_{\phi}(z)), 0)]$$
    $$\max_{\phi} \mathrm{E}_{z \sim q(z)}[L(D_{\theta}(G_{\phi}(z)), 0)]$$
  </p>
  <p>
    The original GAIL algorithm of Ho et al. <dt-cite key="GAIL"></dt-cite> requires that both states $s$ and corresponding actions $a$ are present in the expert policy. In a real life imitation learning problem, however, such as humanoid motion, the actions (joint torques) are hard to obtain compared to states (joint positions).
  </p>
  <p>
  In a normal GAN there are two loss functions that can be both optimized by regular gradient descent algorithms, separately; one for the discriminator and one for the generator. In GAIL, however, the generator network $G_\theta$ acts as a policy $\pi_g$, which requires a policy gradient method. Based on the rollouts $\{z^g_t\}_{t=1\dots T^g}$ of $\pi_g$, and the rewards $\{r_t\}_{t=1\dots T^g}$ associated with each $z^g_t$ that are calculated by the discriminator, the policy gradient method is able to update $\theta$. Contrary to regular GAN practice, the rewards calculated by the discriminator are found to yield better results when $r_t = -\log(1 - D_\phi(\cdot))$ was used compared to $\log(D_\phi(\cdot))$<dt-cite key="DeepMindGAIL"></dt-cite>.

  The loss function to be optimized for the discriminator $D_\phi$ is much like in a regular GAN:
  $$\mathscr{L}(\phi) = \Sigma_{t=1\dots T^g} \log(1 - D_\phi(z^g_t)) - \Sigma_{t=1\dots T^d} \log(D_\phi(z^d_t))$$ After the generator's paramters $\theta$ are updated, the discriminator's parameters $\phi$ are updated $M$ times. Algorithm 1 gives pseudocode of the GAIL policy training.

  </p>
  <p>
    <div style="white-space: pre; font-family: Georgia, serif; border-left:5px solid #ccc; padding: -6px -6px">   <b>Algorithm 1</b>
    <b>Input:</b> set of demonstrations $\{z^d_t\}_{t=1\dots T^d}$
    Initialize $\pi_g$ and $D_\phi$ randomly
    <b>for</b> $i$ in $1 \dots N$ <b>do</b>
        Sample rollouts $\{z^g_t\}_{t=1\dots T^g}$ from $\pi_g$ in the environment $E$
        Calculate rewards $\{r_t = - \log(1 - D_\phi(z^g_t))\}_{t=1\dots T^g}$
        Update $\theta$ with <i>TRPO</i>
        <b>for</b> $j$ in $1\dots M$ <b>do</b>
            $\mathscr{L}(\phi) = \Sigma_{t=1\dots T^g} \log(1 - D_\phi(z^g_t)) - \Sigma_{t=1\dots T^d} \log(D_\phi(z^d_t))$
            Update $\phi$ with gradient descent
    <b>Return:</b> $\pi_g$</div>
  </p>
  <hr>
  <h2>Environment $E$</h2>
    <p>
      Both <a href="http://mujoco.org/">MuJoCo</a> and <a href="https://github.com/openai/roboschool">OpenAI Roboschool</a> are used as physics engines for simulation environments, in which actions $a$ can be taken and the state $s$ can be sampled at any moment:
      <ul>
        <li>
          MuJoCo, a physics engine developed at the University of Washington, is a well respected and used simulator in robotics research labs, but requires a paid license (though free student licenses for personal and class work are available).
        </li>
        <li>
          Roboschool, based on the open-source Bullet Physics Engine, tackles this barrier by removing this license constraint, while supporting the same skeleton definitions as MuJoCo.
        </li>
      </ul>
    </p>
    <p>
      We used a custom made skeleton that makes up a good representation of a real human body in terms of relative length and weight of the joints. The skeleton used both in MuJoCo and Roboschool is shown in Figure XXX.
    </p>
    <p>
      Note that this skeleton is not necessarily exactly the same as the one used in the expert policy. The imitation learning evaluation and rollout is based solely on the states of five vectors representing the 3D end effector positions of the feet, hands and head. The generated policy $\pi_G$ does take actions on all 5X joints.
    </p>
  <hr>
  <h2>Generated Policy $\pi_g$ <font color="red">(Needs work)</font></h2>
    <p>
      The policy network consists of a 3 layer network with neuron widths of (300, 200, 100). The size of this network scales proportionally with the degrees of freedom present in the target skeleton.
    </p>
  <hr>
  <h2>Expert Policy $\pi_d$</h2>
    <p>
      A fundamental component in the system is the skill which needs to be learned. This skill can be any movement of the human body. Such a skill can be seen as a rollout (series of states $\{s_1, s_2, \ldots,s_n\}$ at discrete timesteps $\{t_1, t_2, \ldots, t_n\}$) of the implicit expert policy $\pi_E$ which has a state $s$ to action $a$ mapping. With an increasing level of complexity the system accepts expert input from three sources:  
      <ol>
        <li>Reinforcement Learned Policy</li>
        <li>Motion Capture Data</li>
        <li>Video of Skill</li>
      </ol> 
    </p>
    <p>
      These three different rollout sources give rise to important complexity complications and have their own challenges, which will be discussed in detail below.
    </p>
    <h3>Reinforcement Learned Policy</h3>
      <p>
        A <i>'vanilla'</i> way of using the GAIL model is to imitate a policy that is obtained through <i>Reinforcement Learning (RL)</i>. An agent learns what is best to do (take actions $a$) in an environment based on a given reward function. This method is employed in the original GAIL paper of Ho et al. <dt-cite key="GAIL"></dt-cite>. Note that a RL policy, next to states, also contains actions, which are mostly not observable in a real world setting. The results of our Trust Region Policy Optimization (TRPO) RL policy and its learned GAIL policy can be found in Video XXX.
      </p>
      <p>
      <video width="100%" controls autoplay loop>
        <source src="videos/GAIL_TRPO.mp4" type="video/mp4">
        Your browser does not support the video tag.
      </video>
      Caption lala
    </p>
    <h3>Learning from Motion Capture Data</h3>
      <p>
      A setting much closer to a humanoid reality is to imitate Motion Capture data. Here the input rollout source does not provide any actions nor rewards. This data is obtained through putting markers on actors who then perform a series of skills. The CMU Graphics Lab's Motion Capture Database (MOCAP) provides several skills such as walking and jumping <dt-cite key="MOCAP"></dt-cite>. As this data only provides states, the GAIL is adapted to function without expert actions available as in Merel et al. <dt-cite key="DeepMindGAIL"></dt-cite>.
      </p>

  <p>
  In contrast with a skill rollout, which can be observed in real-life, a RL policy contains a full mapping of every state to every action.  
  </p>
  <h3>Learning from Video</h3>

  <p>A first step towards In this classical setting an agent in an environment is trains a policy of operation using a reinforcement learning algorithm. In our case we opted for Trusted Region Policy Optimization (TRPO) which has a parallellized implementation in Baselines. We trained our modified MOCAP skeleton using TRPO for 10.000 iterations. The result after XXX (1900) iterations is shown below:</p>
  <hr>
  <h2>GAIL as Inverse Reinforcement Learning</h2>
  <p>
    Although GAIL does not use an explicit reward function to evaluate the generated policy, the discriminator does give a reward signal in some sense. As discussed in Finn et al <dt-cite key="FinnGANIRL"></dt-cite>, a Generative Adversarial Network and by extension the GAIL method can be viewed as an Inverse Reinforcement Learning problem.
  </p>
  <hr>
  <h2>Conclusion and further work <font color="red">(Needs work)</font></h2>
  <p>
    We were able to imitate humanoid motion directly from video, which extends on humanoid imitation on pure motion capture data. This encompasses a large reduction in terms of effort which needs to be done for dataset collection for imitation learning.

    Possible further work: multi-task learning.
  </p>
  <hr>
  <h2>Acknowledgements</h2>
  <p>
    The data used in this project was obtained from <a href="http://mocap.cs.cmu.edu">mocap.cs.cmu.edu</a>.
    The database was created with funding from NSF EIA-0196217.
  </p>
</dt-article>

<dt-appendix class="centered">
</dt-appendix>

<script type="text/bibliography">
  @article{GAIL,
  author    = {Jonathan Ho and
               Stefano Ermon},
  title     = {Generative Adversarial Imitation Learning},
  journal   = {CoRR},
  volume    = {abs/1606.03476},
  year      = {2016},
  url       = {http://arxiv.org/abs/1606.03476},
  archivePrefix = {arXiv},
  eprint    = {1606.03476},
  timestamp = {Wed, 07 Jun 2017 14:42:26 +0200},
  biburl    = {http://dblp.org/rec/bib/journals/corr/HoE16},
  bibsource = {dblp computer science bibliography, http://dblp.org},
  url       = {https://arxiv.org/abs/1606.03476}
}
@article{DeepMindGAIL,
  author    = {Josh Merel and
               Yuval Tassa and
               Dhruva TB and
               Sriram Srinivasan and
               Jay Lemmon and
               Ziyu Wang and
               Greg Wayne and
               Nicolas Heess},
  title     = {Learning human behaviors from motion capture by adversarial imitation},
  journal   = {CoRR},
  volume    = {abs/1707.02201},
  year      = {2017},
  url       = {http://arxiv.org/abs/1707.02201},
  archivePrefix = {arXiv},
  eprint    = {1707.02201},
  timestamp = {Tue, 08 Aug 2017 15:06:57 +0200},
  biburl    = {http://dblp.org/rec/bib/journals/corr/MerelTTSLWWH17},
  bibsource = {dblp computer science bibliography, http://dblp.org}
}
@online{MOCAP,
  author = {CMU Graphics Lab},
  title = {Motion Capture Database},
  url = {http://mocap.cs.cmu.edu/},
  urldate = {2017-11-18}
}
@article{FinnGANIRL,
  author    = {Chelsea Finn and
               Paul Christiano and
               Pieter Abbeel and
               Sergey Levine},
  title     = {A Connection between Generative Adversarial Networks, Inverse Reinforcement
               Learning, and Energy-Based Models},
  journal   = {CoRR},
  volume    = {abs/1611.03852},
  year      = {2016},
  url       = {http://arxiv.org/abs/1611.03852},
  archivePrefix = {arXiv},
  eprint    = {1611.03852},
  timestamp = {Wed, 07 Jun 2017 14:40:23 +0200},
  biburl    = {http://dblp.org/rec/bib/journals/corr/FinnCAL16},
  bibsource = {dblp computer science bibliography, http://dblp.org}
}
</script>
