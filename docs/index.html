<!doctype html>
<meta charset="utf-8">
<script src="https://distill.pub/template.v1.js"></script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({ tex2jax: {inlineMath: [["$","$"],["\\(","\\)"]]} });
</script>
<style>
video {
    width: 100% !important;
    height: auto !important;
}

dt-banner {
    display: none !important;
}
</style>
<script type="text/javascript" src="https://www.tuhh.de/MathJax/MathJax.js?config=TeX-AMS_HTML-full"></script>
<script type="text/front-matter">
    title: "Human Behavior Imitation Learning"
    description: "Description of the post"
    authors:
      - Shoubhik Debnath: https://robotics.usc.edu/resl/
      - Arnout Devos: https://robotics.usc.edu/resl/
      - Eric Heiden: https://robotics.usc.edu/resl/
      - Ryan Julian: https://robotics.usc.edu/resl/
      - Fiona Khatana: https://robotics.usc.edu/resl/
    affiliations:
      - USC: http://usc.edu
      - USC: http://usc.edu
      - USC: http://usc.edu
      - USC: http://usc.edu
      - USC: http://usc.edu
</script>
<dt-article class="centered" style="text-align:justify">
    <h1>Humanoid Imitation Learning from Video</h1>
    <h2 style="text-align:justify">This post describes our experience implementing a system which learns movement skills for humanoid figures from imitation, and all of the supporting infrastructure and data processing necessary to do so. Our system employs a generative adversarial imitation learning (GAIL) architecture, which is a type of generative adversarial network. We successfully trained our GAIL to control a custom-designed humanoid skeleton, using expert demonstrations from an RL policy learned using that skeleton. We also explored several methods for deriving humanoid skeleton demonstrations from video data, developed a preprocessing pipeline for motion capture data. We hope to refine our system in the future to learn from motion capture data and directly from videos of human walking. Our system is a work-in-progress, which is the foundation for several possible future research projects.
    </h2>
    <dt-byline></dt-byline>
    <h2>Introduction to GAIL</h2>
    <p>
      <img src="architecture.svg" alt="Architecture diagram" style='height: 100%; width: 100%; object-fit: contain'>
      Architecture diagram for our GAIL imitation learning system.
    </p>
    <p>
      $$\min_{\theta} \mathrm{E}_{s,a \sim p(s,a)}[L(D_{\theta}(s,a), 1)] + \mathrm{E}_{z \sim q(z)}[L(D_{\theta}(G_{\phi}(z)), 0)]$$ $$\max_{\phi} \mathrm{E}_{z \sim q(z)}[L(D_{\theta}(G_{\phi}(z)), 0)]$$
    </p>
    <p>
        Generative adversarial imitation learning (GAIL) is a deep neural network architecture for imitation learning, wherein an agent ("learner") learns a skill by observing the behavior of another agent ("expert"). It is based on the popular generative adversarial network (GAN) architecture, in which the network is divided into two principal functional blocks engaged in an adversarial game: the "discriminator" (or "critic") learns to distinguish real training examples from examples created by the network, while the "generator" (or "actor") block learns to produce convincing counterfeits of real examples meant to fool the discriminator. Those counterfeits are also the useful output of a well-trained network.
        <h4>GAIL as Inverse Reinforcement Learning</h4>
        <p>
            Although GAIL does not use an explicit reward function to evaluate the generated policy, the discriminator does give a reward signal in some sense. As discussed in Finn et al
            <dt-cite key="FinnGANIRL"></dt-cite>, a Generative Adversarial Network and by extension the GAIL method can be viewed as an Inverse Reinforcement Learning problem.
        </p>
    </p>
    <p>
        A key fact for applying GANs to imitation learning is that the generator is never exposed to real world training examples, only the discriminator. In GAIL, the discriminator learns to distinguish generated performances from expert demonstrations, while the generator attempts to mimic the expert convincingly-enough to fool the discriminator into thinking its performance was an expert demonstration. In our use of GAIL, the expert demonstrations are time-series humanoid motion traces from a variety of sources (motion capture, video, artificially-trained simulations), and the output of the generator is a policy (a function mapping from states $s$ to actions $a$) for moving a humanoid model in simulation to mimic those demonstrated motions.
    </p>
    <p>
        The original GAIL formulation introduced by Ho et al.
        <dt-cite key="GAIL"></dt-cite> proposes that expert demonstrations containing both states $s$ and corresponding actions $a$ are presented to the discriminator. In a real life imitation learning problem, such as humanoid motion, the actions (e.g. joint torques) are difficult to obtain compared to states (e.g. joint positions). Our work attempts to address this and other practical problems in using GAIL for imitation learning outside simulated environments.
    </p>
    <p>
        In the canonical GAN formulation there are two loss functions that can be both optimized by regular gradient descent algorithms, separately. One loss function for the discriminator and one for the generator. In GAIL, however, the generator network $G_\theta$ encodes a policy $\pi_g$ and cannot be trained with simple gradient descent. We use a policy gradient method to train the generator, though there are also non-gradient methods explored in the literature. Based on the rollouts $\{z^g_t\}_{t=1\dots T^g}$ of $\pi_g$, and the rewards $\{r_t\}_{t=1\dots T^g}$ associated with each $z^g_t$ that are calculated by the discriminator, the policy gradient method is updates $\theta$, the parameters representing the policy function. Also contrary to typical GAN practice, the reward input to policy gradient, as calculated by the discriminator, is found in the literature to yield better results when presented as $r_t = -\log(1 - D_\phi(\cdot))$ as opposed to $\log(D_\phi(\cdot))$
        <dt-cite key="DeepMindGAIL"></dt-cite>. The loss function to be optimized for the discriminator $D_\phi$ is much like in a regular GAN: $$\mathscr{L}(\phi) = \Sigma_{t=1\dots T^g} \log(1 - D_\phi(z^g_t)) - \Sigma_{t=1\dots T^d} \log(D_\phi(z^d_t))$$ After the generator's parameters $\theta$ are updated, the discriminator's parameters $\phi$ are updated $M$ times. Algorithm 1 gives pseudocode of the GAIL policy training.
    </p>
    <p>
        <div style="white-space: pre; font-family: Georgia, serif; border-left:5px solid #ccc; padding: -6px -6px"> <b>Algorithm 1</b>
            <b>Input:</b>
              set of demonstrations $\{z^d_t\}_{t=1\dots T^d}$ Initialize $\pi_g$ and $D_\phi$ randomly
            <b>for</b> $i$ in $1 \dots N$ <b>do</b>
              Sample rollouts $\{z^g_t\}_{t=1\dots T^g}$ from $\pi_g$ in the environment $E$
              Calculate rewards $\{r_t = - \log(1 - D_\phi(z^g_t))\}_{t=1\dots T^g}$
              Update $\theta$ with <i>TRPO</i>
            <b>for</b> $j$ in $1\dots M$ <b>do</b>
              $\mathscr{L}(\phi) = \Sigma_{t=1\dots T^g} \log(1 - D_\phi(z^g_t)) - \Sigma_{t=1\dots T^d} \log(D_\phi(z^d_t))$
              Update $\phi$ with gradient descent
            <b>Return:</b> $\pi_g$</div>
    </p>
    <hr>
    <h2>Simulation Environment $E$ and Target Skeleton</h2>
    <p>
        We developed support for both <a href="http://mujoco.org/">MuJoCo</a> and <a href="https://github.com/openai/roboschool">OpenAI Roboschool</a> as physics engines for simulation environments:
        <ul>
            <li>
                MuJoCo, a physics engine developed at the University of Washington, is a well respected and used simulator in robotics research labs, but requires a paid license (though free student licenses for personal and class work are available).
            </li>
            <li>
                Roboschool, based on the open-source Bullet Physics Engine, tackles this barrier by removing this license constraint, while supporting the same skeleton definitions as MuJoCo.
            </li>
        </ul>
        <p>
            Our original ambition was to use Roboschool exclusively for our target humanoid environment. However, we found that our implementation of TRPO, based on Open AI Baselines, is highly-tuned for MuJoCo and failed to train in Roboschool environments. All experiments detailed herein used MuJoCo as the simulation environment.
        </p>
        <p>
            We engineered a custom MuJoCo humanoid skeleton in an attempt to approximate of a real human body in terms of relative length and weight of the joints. The skeleton used both is shown in the figure below. Note that this skeleton is not the same as the one used in the expert policy, and so our implementation performs imitation as well as retargeting. The imitation learning evaluation and rollout is based solely on the five 3D vector positions (relative to the pelvis) of the feet, hands, and head. The generated policy $\pi_G$ take actions on all 21 joints.
        </p>
        <h2>Motion Capture Extraction and Resampling</h2>
        <p>
          The CMU motion capture dataset provides AMC files which contain the joint angle definitions for various motions (e.g. walking, dancing, etc.) of the full humanoid skeleton at a frequency of 120Hz. We created an automated tool to intepret the AMC files, down-sample data to the same frame rate as our simulator, and generate the rollouts from motion capture data. For this project we down-sampled the animation to a frequency of approximately 33.33Hz using cubic spline interpolation.
        </p>
        <p>
          <video width="100%" controls autoplay loop>
            <source src="videos/Dance_120HZ.mp4" type="video/mp4">
                  Your browser does not support the video tag.
          </video>
          Dancing at 120Hz from motion capture data
        </p>

        <p>
          <video width="100%" controls autoplay loop>
            <source src="videos/Dance_33HZ.mp4" type="video/mp4">
              Your browser does not support the video tag.
          </video>
          Dancing at 33.33Hz from motion capture data after cubic spline interpolation
        </p>
        <h2>Generator Policy Network $\pi_g$</h2>
        <p>
            Our final policy network consists of a 3 layer network with neuron widths of (150, 150, 150). We also experimented with dual layer networks of hidden shape (32, 32) and (64, 64), but found the larger network necessary for reliable performance. We found that the size of the network necessary for adequate performance scales proportionally with the degrees of freedom present in the target skeleton. Unfortunately, larger networks also slowed training significantly.
        </p>
        <hr>
        <h2>Expert Policy $\pi_d$</h2>
        <p>
            The expert skill can be any movement of the human body. Such a skill can be represented as a rollout (series of states $\{s_1, s_2, \ldots,s_n\}$ at discrete timesteps $\{t_1, t_2, \ldots, t_n\}$) generated from the implicit expert policy $\pi_E$, which is a function mapping the state of the expert $s$ to its next action from that state $a$. We attempted to train the system from three sources of expert demonstrations, presented in order of increasing complexity.
            <ol>
                <li>Reinforcement Learned Policy</li>
                <li>Motion Capture Data</li>
                <li>Video of Skill</li>
            </ol>
        </p>
        <p>
            These three different rollout sources give rise to important complications and training from each has their own challenges, which we discuss in detail below.
        </p>
        <h3>Learning from Artificial Experts (RL)</h3>
        <p>
            The simplest way of testing a GAIL is to imitate a policy obtained through direct <i>Reinforcement Learning (RL)</i>, in which the agent learns to perform skills in an environment based on a given reward function by attempting to maximize a reward function. This method is employed in the original GAIL paper of Ho et al.
            <dt-cite key="GAIL"></dt-cite>. Note that an RL policy provides both states and actions, but actions are difficult to observe in the real world. The results of our Trust Region Policy Optimization (TRPO) RL policy and its learned GAIL policy are shown in the video below.
        </p>
        <p>
            <video width="100%" controls autoplay loop>
                <source src="videos/GAIL_TRPO.mp4" type="video/mp4"> Your browser does not support the video tag.
            </video>
            Side-by-side comparison of learned humanoid walking policies from the RL and GAIL algorithms. We trained the RL policy (left) using TRPO, then used it to provide expert demonstrations for training the GAIL network (right).
        </p>
        <h3>Learning from Motion Capture Data</h3>
        <p>
            A setting much closer to reality is to imitate motion capture data. Here the input rollout source does not provide any actions nor rewards. Motion capture data is obtained by attaching trackable markers to actors, who then perform recorded skill demonstrations inside a special tracking system. The CMU Graphics Lab's Motion Capture Database (MOCAP) provides several skills such as walking and jumping
            <dt-cite key="MOCAP"></dt-cite>. As this data only provides states, the GAIL we adapted the GAIL to function without expert actions available, as in Merel et al.
            <dt-cite key="DeepMindGAIL"></dt-cite>.
        </p>
        <h3>Learning from Video</h3>
        <p>
            Learning from motion capture data requires putting markers on actors who demonstrate various skills, and asking them for perform in an expensive tracking system. We would like to instead learn skills directly from video. The first step to achieve this, within our existing GAIL framework, is generate rollouts of three dimensional human poses while performing a skill. Most approaches of obtaining a three-dimensional pose from raw images belongs to one of two categories:
        </p>
        <ol>
            <li>Using a pipeline approach, where first a 2D pose is estimated from an image. Followed by an estimation of the 3D pose from the generated 2D pose.</li>
            <li>Learning 3D poses directly from images.</li>
        </ol>
        <p>
            We first tried the pipeline approach using a model trained for 2D pose estimation
            <dt-cite key="cao2017realtime"></dt-cite>, and fed the output of that model to one which estimates 3D pose from 2D pose
            <dt-cite key="martinez_2017_3dbaseline"></dt-cite>. We were able to achieve good results for 2D pose estimation, but 3D pose estimation suffered from restricted movement for skills like walking. The brittleness of connecting two imperfect models taught us why end-to-end approaches are increasingly popular for reconstruction problems like this one.
        </p>
        <p>
            <video width="100%" controls autoplay loop>
                <source src="videos/VID2D.mov" type="video/mp4"> Your browser does not support the video tag.
            </video>
            Video: 2D pose results for Approach 1
        </p>
        <p>
            Next, we experimented with methods which directly learn 3D poses from images using a probabilistic 3D human pose model. This overcomes the limitation of the pipeline approach by backpropagating 3D information about the skeletal structure to the 2D convolutional layers. In this way, the prediction of 2D pose benefits from the 3D information encoded
            <dt-cite key="Tome_2017_CVPR"></dt-cite>. This approach works in two stages. Firstly, belief maps obtained from the previous stage $t−1$ are used to predict an updated set of belief maps for the 2D human joint positions. Secondly, the output of the CNN based belief maps is taken as input to a new layer that uses a pretrained probabilistic 3D human pose model to lift the proposed 2D poses into 3D. This approach gave us better 3D pose estimations.
        </p>
        <p>
            We then extend this approach from images to videos. This is done by first obtaining frames from a rollout for a given skill like walking, running etc. The results for this approach can be seen below:
        </p>
        <p>
            <video width="100%" controls autoplay loop>
                <source src="videos/walking_better.mov" type="video/mp4"> Your browser does not support the video tag.
            </video>
            Pose estimation example for walking. Left: Original video and overlayed 2D pose estimate. Right: 3D pose estimate
        </p>
        <p>
            <video width="100%" controls autoplay loop>
                <source src="videos/run_demo.mp4" type="video/mp4"> Your browser does not support the video tag.
            </video>
            Pose estimation example for running. Left: Original video and overlayed 2D pose estimate. Right: 3D pose estimate
        </p>
        <hr>
        <h2>Achievements</h2>
        <p>
          We implemented a deep RL development environment and tools for doing our research. We also trained expert RL policies, train GAIL policies from those experts, and attempted (unsuccessfully, so far) to train GAIL policies from motion capture and video. We also implemented two approaches for human pose reconstruction from video, using three pretrained networks from the literature. We hope plan on continuing this research in the future.
          <ul>
            <li>Implemented deep RL training environment using Docker, with support for GPUs, OpenGL visualization, TensorFlow, MuJoCo, Roboschool, Jupyter Notebook, and many other tools. The environment runs on Windows, Mac, and Linux and is easy to use with cloud computing providers.</li>
            <li>Learned how to use Google Cloud to train on many CPUs at once</li>
            <li>Created custom instrumented humanoid skeleton and training environment in MuJoCo and Roboschool</li>
            <li>Implemented motion capture data interpreter and time-series resampling pipeline</li>
            <li>Trained humanoid walking policies on a custom skeleton using TRPO</li>
            <li>Trained GAIL walking policies using TRPO-based expert</li>
            <li>Implemented humanoid policy rollout visualizer with feature overlays</li>
            <li>Collected human walking video demonstrations</li>
            <li>Experimented with video→2D pose→3D pose pipeline approach for human pose estimation from video, using two pretrained networks from the literature</li>
            <li>Experimented with end-to-end video→3D approach for human pose estimation from video, using a pretrained network from the literature</li>
            <li>Experimented with GAIL target humanoid, policy network shape, and rollout feature representations to attain a stable GAIL using RL policy as expert</li>
          </ul>
        </p>
        <h2>Lessons Learned</h2>
        <p>
          <ul>
            <li>Most deep RL software is very young and tends to be brittle or highly-tuned for one environment (e.g. MuJoCo and TRPO)</li>
            <li>GAIL, like all GANs, tends to be very unstable and hard to train</li>
            <li>Ingesting outside data sources required a substantial amount of work in interpreting and preprocessing the data</li>
            <li>Pipeline approaches are simpler to understand and train, but every step in the pipeline creates a failure mode. End-to-end approaches allow the learner to backpropagate failures through the pipeline to compensate, and can be more robust.
            <li>The formulation of GAIL for humanoids from Merel et al. <dt-cite key="DeepMindGAIL"></dt-cite> is very brittle to the target humanoid and the feature representation chosen for rollouts<. This is an opportunity for future work.</li>
            <li>Training deep RL policies with high-dimensional agents like humanoids requires very large amounts of CPU-based computing</li>
          </ul>
        </p>
        <h2>Conclusion and future work</h2>
        <p>
            Our goals in this project was to familiarize ourselves with state-of-the-art deep RL software and research techniques, while reimplementing previous work and laying a foundation for future research. While we did not successfully reimplement the results from <dt-cite key="DeepMindGAIL"></dt-cite>, we certainly achieved our two other goals. We now have a deep RL training environment and workflow, and several future research directions we hope to pursue and publish in peer-reviewed forums.
        </p>
        <p>
            Possible directions include:
            <ul>
              <li>One-shot imitation learning from video using a GAIL network pretrained from a simulated expert</li>
              <li>Learning feature representations for the expert demonstrations to avoid feature engineering for each new target morphology</li>
              <li>Exploring architectures and roll-out representations (e.g. Fourier coefficients, etc.) which are more effective for time-domain motion data</li>
              <li>Using GAIL as the foundation for a multi-task imitation learner</li>
            </ul>
        </p>
        <hr>
        <h2>Acknowledgements</h2>
        <p>
            We would like to express our appreciation to <a href="https://andrewliao11.github.io/">Andrew Liao</a> for providing the TensorFlow-based GAIL implementation used in this research. We would also like to thank our TAs Aartem Molchanov and Shao-Hua Sun, and Prof. Joseph Lim for their guidance and support. The motion capture data used in this research was obtained from <a href="http://mocap.cs.cmu.edu">mocap.cs.cmu.edu</a>. The database was created with funding from NSF EIA-0196217.
        </p>
</dt-article>
<dt-appendix class="centered">
</dt-appendix>
<script type="text/bibliography">
@article{GAIL,
  author    = {Jonathan Ho and
               Stefano Ermon},
  title     = {Generative Adversarial Imitation Learning},
  journal   = {CoRR},
  volume    = {abs/1606.03476},
  year      = {2016},
  url       = {http://arxiv.org/abs/1606.03476},
  archivePrefix = {arXiv},
  eprint    = {1606.03476},
  timestamp = {Wed, 07 Jun 2017 14:42:26 +0200},
  biburl    = {http://dblp.org/rec/bib/journals/corr/HoE16},
  bibsource = {dblp computer science bibliography, http://dblp.org},
  url       = {https://arxiv.org/abs/1606.03476}
}
@article{DeepMindGAIL,
  author    = {Josh Merel and
               Yuval Tassa and
               Dhruva TB and
               Sriram Srinivasan and
               Jay Lemmon and
               Ziyu Wang and
               Greg Wayne and
               Nicolas Heess},
  title     = {Learning human behaviors from motion capture by adversarial imitation},
  journal   = {CoRR},
  volume    = {abs/1707.02201},
  year      = {2017},
  url       = {http://arxiv.org/abs/1707.02201},
  archivePrefix = {arXiv},
  eprint    = {1707.02201},
  timestamp = {Tue, 08 Aug 2017 15:06:57 +0200},
  biburl    = {http://dblp.org/rec/bib/journals/corr/MerelTTSLWWH17},
  bibsource = {dblp computer science bibliography, http://dblp.org}
}
@article{Bailey2016,
  author = {Bailey, Stephen and Lin, Chen},
  url = {https://bcourses.berkeley.edu/files/70256985/download?download_frd=1&verifier=Dm4iKJcQ6815wJQygi04eH8iRz1z5HrbeSaz22ee},
  title = {{Improving Appearance of Simulated Humanoid Locomotion}},
  year = {2016}
}
@online{MOCAP,
  author = {CMU Graphics Lab},
  title = {Motion Capture Database},
  url = {http://mocap.cs.cmu.edu/},
  urldate = {2017-11-18}
}
@article{FinnGANIRL,
  author    = {Chelsea Finn and
               Paul Christiano and
               Pieter Abbeel and
               Sergey Levine},
  title     = {A Connection between Generative Adversarial Networks, Inverse Reinforcement
               Learning, and Energy-Based Models},
  journal   = {CoRR},
  volume    = {abs/1611.03852},
  year      = {2016},
  url       = {http://arxiv.org/abs/1611.03852},
  archivePrefix = {arXiv},
  eprint    = {1611.03852},
  timestamp = {Wed, 07 Jun 2017 14:40:23 +0200},
  biburl    = {http://dblp.org/rec/bib/journals/corr/FinnCAL16},
  bibsource = {dblp computer science bibliography, http://dblp.org}
}
@inproceedings{cao2017realtime,
  author = {Zhe Cao and Tomas Simon and Shih-En Wei and Yaser Sheikh},
  booktitle = {CVPR},
  title = {Realtime Multi-Person 2D Pose Estimation using Part Affinity Fields},
  url = {https://arxiv.org/pdf/1611.08050.pdf},
  year = {2017}
}
@inproceedings{martinez_2017_3dbaseline,
  title={A simple yet effective baseline for 3d human pose estimation},
  author={Martinez, Julieta and Hossain, Rayat and Romero, Javier and Little, James J.},
  booktitle={ICCV},
  url = {https://arxiv.org/pdf/1705.03098.pdf},
  year={2017}
}
@InProceedings{Tome_2017_CVPR,
  author = {Tome, Denis and Russell, Chris and Agapito, Lourdes},
  title = {Lifting From the Deep: Convolutional 3D Pose Estimation From a Single Image},
  booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  url = {http://openaccess.thecvf.com/content_cvpr_2017/papers/Tome_Lifting_From_the_CVPR_2017_paper.pdf},
  month = {July},
  year = {2017}
}
</script>
