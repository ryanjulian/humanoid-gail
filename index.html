<!doctype html>
<meta charset="utf-8">
<script src="https://distill.pub/template.v1.js"></script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {inlineMath: [["$","$"],["\\(","\\)"]]}
  });
</script>
<script type="text/javascript" src="https://www.tuhh.de/MathJax/MathJax.js?config=TeX-AMS_HTML-full"></script>

<script type="text/front-matter">
  title: "Human Behavior Imitation Learning"
  description: "Description of the post"
  authors:
  - Shoubhik Debnath: http://colah.github.io
  - Arnout Devos: http://shancarter.com
  - Eric Heiden: http://shancarter.com
  - Ryan Julian: http://shancarter.com
  - Fiona Khatana: http://shancarter.com
  affiliations:
  - USC: http://usc.edu
  - USC: http://usc.edu
  - USC: http://usc.edu
  - USC: http://usc.edu
  - USC: http://usc.edu
</script>

<dt-article>
  <h1>Humanoid Imitation Learning from Video</h1>
    <h2>This article describes a system which demonstrates imitation learning for useful skills on humanoid figures. The system employs a Generative Adversarial Imitation Learning model and is able to imitate directly from videos of human skill, without explicit reward.</h2>
  <dt-byline></dt-byline>
  <p>
    This is the first paragraph of the article.
  </p>
  <p>
    $$\min_{\theta} \mathrm{E}_{s,a \sim p(s,a)}[L(D_{\theta}(s,a), 1)] + \mathrm{E}_{z \sim q(z)}[L(D_{\theta}(G_{\phi}(z)), 0)]$$
    $$\max_{\phi} \mathrm{E}_{z \sim q(z)}[L(D_{\theta}(G_{\phi}(z)), 0)]$$
  </p>
  <p>
    The original GAIL algorithm of Ho et al. <dt-cite key="GAIL"></dt-cite> requires that both states $s$ and corresponding actions $a$ are present in the expert policy. In a real life imitation learning problem, however, such as humanoid motion, the actions (joint torques) are hard to obtain compared to states (joint positions).
  </p>
  <hr>
  <h2>Expert Policy $\pi_E$</h2>
    <p>
      A fundamental component in the system is the skill which needs to be learned. This skill can be any movement of the human body. Such a skill can be seen as a rollout (series of states $\{s_1, s_2, \ldots,s_n\}$ at discrete timesteps $\{\}$) of the implicit expert policy $\pi_E$ which has a state $s$ to action $a$ mapping. With an increasing level of complexity the system accepts expert input from three sources:  
      <ol>
        <li>Reinforcement Learned Policy</li>
        <li>Motion Capture Data</li>
        <li>Video of Skill</li>
      </ol> 
    </p>
    <p>
      These three different rollout sources give rise to important complexity complications and have their own challenges, which will be discussed in detail below.
    </p>
    <h3>Reinforcement Learned Policy</h3>
      <p>
        A <i>'vanilla'</i> way of using the GAIL model is to imitate a policy that is obtained through <i>Reinforcement Learning (RL)</i>. This method is employed in the original GAIL paper of Ho et al. <dt-cite key="GAIL"></dt-cite>. Note that a RL policy, next to states, also contains actions, which are mostly not observable in a real world setting. The results of our RL policy and its learned GAIL policy can be found in Video XXX.
      </p>
    <h3>Learning from Motion Capture Data</h3>
      <p>
      A setting which is much closer to a humanoid reality is to imitate Motion Capture data. This data is obtained through putting markers on actors who then perform a series of skills. The CMU Graphics Lab's Motion Capture Database (MOCAP) provides several skills such as walking and jumping <dt-cite key="MOCAP"></dt-cite>. As this data only provides states, the GAIL is adapted to function without expert actions available as in Merel et al. <dt-cite key="DeepMindGAIL"></dt-cite>.
      </p>

  <p>
  In contrast with a skill rollout, which can be observed in real-life, a RL policy contains a full mapping of every state to every action.  
  </p>
  <h3>Learning from Video</h3>

  <p>A first step towards In this classical setting an agent in an environment is trains a policy of operation using a reinforcement learning algorithm. In our case we opted for Trusted Region Policy Optimization (TRPO) which has a parallellized implementation in Baselines. We trained our modified MOCAP skeleton using TRPO for 10.000 iterations. The result after XXX (1900) iterations is shown below:</p>
  <h2>GAIL as an RL problem</h2>
</dt-article>

<dt-appendix>
</dt-appendix>

<script type="text/bibliography">
  @article{GAIL,
  author    = {Jonathan Ho and
               Stefano Ermon},
  title     = {Generative Adversarial Imitation Learning},
  journal   = {CoRR},
  volume    = {abs/1606.03476},
  year      = {2016},
  url       = {http://arxiv.org/abs/1606.03476},
  archivePrefix = {arXiv},
  eprint    = {1606.03476},
  timestamp = {Wed, 07 Jun 2017 14:42:26 +0200},
  biburl    = {http://dblp.org/rec/bib/journals/corr/HoE16},
  bibsource = {dblp computer science bibliography, http://dblp.org},
  url       = {https://arxiv.org/abs/1606.03476}
}
@article{DeepMindGAIL,
  author    = {Josh Merel and
               Yuval Tassa and
               Dhruva TB and
               Sriram Srinivasan and
               Jay Lemmon and
               Ziyu Wang and
               Greg Wayne and
               Nicolas Heess},
  title     = {Learning human behaviors from motion capture by adversarial imitation},
  journal   = {CoRR},
  volume    = {abs/1707.02201},
  year      = {2017},
  url       = {http://arxiv.org/abs/1707.02201},
  archivePrefix = {arXiv},
  eprint    = {1707.02201},
  timestamp = {Tue, 08 Aug 2017 15:06:57 +0200},
  biburl    = {http://dblp.org/rec/bib/journals/corr/MerelTTSLWWH17},
  bibsource = {dblp computer science bibliography, http://dblp.org}
}
@online{MOCAP,
  author = {CMU Graphics Lab},
  title = {Motion Capture Database},
  url = {http://mocap.cs.cmu.edu/},
  urldate = {2017-11-18}
}
</script>
